
\chapter{Framework}

\section{Unconstrained Non-Markovian Case}


\subsection{Primal Problem}

Let $(\Omega, \mathcal{F}, \mathbb{F}, \mathbb{P})$ be a complete probability space, where $\mathbb{F} := \{ \mathcal{F}_t \}_{t \in [t_0, T]}$ is the $\mathbb{P}$-augmentation of the filtration generated by a $d$-dimensional standard Brownian motion $\bm{W}(t) = (W_1(t), \ldots, W_d(t))$. 

We define the set of admissible controls as:
\[
    \mathcal{A} := \big\{ \pi \in \mathcal{H}([t_0, T], \mathbb{R}^m) \big\}.
\]

Given a control $\pi \in \mathcal{A}$, we consider the $n$-dimensional state process $X(t)$ governed by the stochastic differential equation (SDE):
\begin{equation}
\label{eq:primal_sde}
\begin{cases}
    \mathrm{d}X(t) = \left[ A(t) X(t) + B(t)\pi(t) \right] \mathrm{d}t + \sum_{i=1}^d \left[ C_i(t) X(t) + D_i(t) \pi(t) \right] \mathrm{d}W_i(t), \\
    X(t_0) = x_0 \in \mathbb{R}^n,
\end{cases}
\end{equation}
where $A(t), C_i(t) \in \mathbb{R}^{n \times n}$ and $B(t), D_i(t) \in \mathbb{R}^{n \times m}$ are $\mathbb{F}$-progressively measurable and uniformly bounded.

Define the drift and diffusion coefficients:
\begin{align*}
    b(t, X, \pi) &:= A(t) X + B(t) \pi, \\
    \sigma(t, X, \pi) &:= \big[ C_1(t) X + D_1(t)\pi, \ldots, C_d(t) X + D_d(t)\pi \big] \in \mathbb{R}^{n \times d}.
\end{align*}

Then the SDE \eqref{eq:primal_sde} can be compactly written as:
\begin{equation}
\label{eq:primal_sde_compact}
\begin{cases}
    \mathrm{d}X(t) = b(t, X(t), \pi(t)) \, \mathrm{d}t + \sigma(t, X(t), \pi(t)) \, \mathrm{d}W(t), \\
    X(t_0) = x_0.
\end{cases}
\end{equation}

We aim to minimise the following cost functional:
\begin{equation}
\label{eq:cost_functional}
J(\pi) := \mathbb{E} \left[ \int_{t_0}^T f(t, X(t), \pi(t)) \, \mathrm{d}t + g(X(T)) \right],
\end{equation}
with
\begin{align}
    f(t, X, \pi) &= \tfrac{1}{2} X^\top Q(t) X + X^\top S^\top(t) \pi + \tfrac{1}{2} \pi^\top R(t) \pi, \label{eq:running_cost} \\
    g(X) &= \tfrac{1}{2} X^\top G(T) X + X^\top L(T). \label{eq:terminal_cost}
\end{align}

Assume:
\begin{itemize}
    \item $Q(t), R(t)$ are symmetric, positive semidefinite, and uniformly bounded;
    \item $S(t)$, $G(T)$, and $L(T)$ are appropriately measurable and bounded;
    \item the matrix
    \[
        \begin{pmatrix}
            Q(t) & S^\top(t) \\
            S(t) & R(t)
        \end{pmatrix}
        \succeq 0.
    \]
\end{itemize}

Then $J(\pi)$ is a convex functional over $\mathcal{A}$. The optimisation problem is:
\begin{equation}
    \min_{\pi \in \mathcal{A}} J(\pi) \quad \text{subject to } (X, \pi) \text{ admissible}.
    \label{eq:optimisation_problem}
\end{equation}
A control $\hat{\pi}$ is optimal if it attains the minimum of \eqref{eq:optimisation_problem}.

\subsection{Dual Problem}
We now derive the dual problem. We follow the derivation in \cite{Constrained_Quadratic_Risk_Minimisation_Via_FBSDEs}. First, assume that the dual process $Y$ is driven by an SDE of the form
\begin{equation*}
    \d Y(t) = \alpha_1 (t) + \sum_{i=1}^d \beta_i(t) \d W_i(t),
\end{equation*}
with initial condition $Y(t_0) = y$, where $\alpha_1$ and $\beta_i$ are stochastic processes to be determined. Ito's lemma gives 
\begin{align*}
    \d (X^T Y) &= X^T \d Y + Y^T \d X + \d X^T \d Y\\
    &= \bigg[  X^T \underbrace{\bigg(\alpha_1 + A^T Y + \sum_{i=1}^d C_i^T \beta_i\bigg)}_{:= \alpha} + \pi^T \underbrace{\bigg(B^T Y + \sum_{i=1}^d D_i^T \beta_i\bigg)}_{:= \beta} \bigg] \d t + \text{local martingale}.
\end{align*}
Denoting $\alpha = \alpha_1 + A^T Y + \sum_{i=1}^d C_i^T \beta_i$ and $\beta = B^T Y + \sum_{i=1}^d D_i^T \beta_i$, we have that the dual process $Y(t)$ satisfies
\begin{equation}
    \begin{cases}
        \d Y(t) &= \big[ \alpha(t) - A(t)^T Y(t) - \sum_{i=1}^d C_i(t)^T \beta_i(t)\big]\d t + \sum_{i=1}^d \beta_i(t) \d W_i(t)\\
        Y(t_0) &= y. \label{eq: y_sde}
    \end{cases}
\end{equation}
There is a unique solution to the SDE for given $(y, \alpha, \beta_1, \dots, \beta_d)$. We call $(\alpha, \beta_1, \dots, \beta_d)$ the \textit{admissible dual control} and $(Y, \alpha, \beta_1, \dots, \beta_d)$ the \textit{admissible dual pair}. Returning to Ito's lemma to $X(t)^T Y(t)$, we have
\begin{align*}
    \d (X^T Y) &= (X^T \alpha + \pi^T \beta ) \d t + \text{local martingale}.
\end{align*}
The process $X^T(t)Y(t) - \int_{t_0}^t [X^T(s) \alpha(s) + \pi^T(s) \beta(s)] \d s$ is a local martingale and a supermartingale if it is bounded below by an integrable process, which gives 
\begin{equation}
    \E \bigg[ X^T(T) Y(T) - \int_{t_0}^T \big(X^T(s) \alpha(s) + \pi^T(s) \beta(s) \big) \d s \bigg] \le X^T(t_0) Y(t_0) =  x^T y. \label{eq: dual_1}
\end{equation}
The optimisation problem \eqref{eq: minimisation_problem} can be written equivalently as 
\begin{equation*}
    \sup_\pi \E \bigg[ -\int_{t_0}^T f(t,X(t), \pi(t)) \d t - g(X(T)) \bigg].
\end{equation*}
Define the dual functions $\phi : [t_0, T] \times \R^n \times \R^m \to \R$ by
\begin{equation}
    \phi(t, \alpha, \beta) = \sup_{x, \pi} \big\{x^T \alpha + \pi^T \beta - f(t, x, \pi) \big\} \label{eq: phi_1}
\end{equation}
and $h: \R^n \to \R$ by
\begin{equation}
    h(y) = \sup_x \big\{-x^T y - g(x)\big\}. 
    \label{eq: h_1}
\end{equation}

Combining \eqref{eq: dual_1}, \eqref{eq: phi_1}, \eqref{eq: h_1}, we get the following inequality:
\begin{equation}
    \sup_\pi \E \bigg[ -\int_{t_0}^T f(t, X,\pi) \d t - g(X(T)) \bigg] \le \inf_{y, \alpha, \beta_1, \dots, \beta_d} \bigg[ x^T y + \E\bigg[\int_{t_0}^T \phi(t,\alpha, \beta ) \d t + h(Y(T)) \bigg] \bigg]. \label{eq: dual_primal_inequality}
\end{equation}
The dual control problem is defined by
\begin{equation}
    \inf_{y, \alpha, \beta_1, \dots, \beta_d} \bigg[ x^T y + \E\bigg[\int_{t_0}^T \phi(t,\alpha, \beta) \d t + h(Y(T)) \bigg] \bigg], \label{eq: dual_control_problem}
\end{equation}
where $Y$ satisfies \eqref{eq: y_sde}. Problem \eqref{eq: dual_control_problem} can be solved in two steps: first, for fixed $y$, solve a stochastic control problem
\begin{equation}
    - \tilde{v}(t, y) = \inf_{ \alpha, \beta_1, \dots, \beta_d} \E\bigg[\int_{t_0}^T \phi (t, \alpha, \beta) \d t + h(Y(T)) \bigg], \label{eq: dual_value_function}
\end{equation}
and second, solve a static optimisation problem
\begin{equation*}
    \inf_y (x^T y - \tilde{v}(t, y)).
\end{equation*}
To find $\phi$ and $h$, we substitute $f$ and $g$ from \eqref{eq: f} and \eqref{eq: g} into \eqref{eq: phi_1} and \eqref{eq: h_1} and we can find the supremums by setting the derivatives to zero. We get
\begin{equation*}
    \phi(t, \alpha, \beta) = \sup_{x, \pi} \bigg\{
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}^T
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix} - \frac12
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}^T
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}
    \bigg\},
\end{equation*}
so setting the derivative to zero, we get
\begin{equation*}
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix} - 
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}
    = 0 \implies 
    \begin{bmatrix}
        x^\ast\\
        \pi^\ast
    \end{bmatrix} = 
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix}.
\end{equation*}
Therefore
\begin{align*}
    \pi^\ast &= \big[ S Q^{-1}S^T - R \big]^{-1}(S Q^{-1} \alpha - \beta)\\
    x^\ast &= Q^{-1} (\alpha - S^T \pi^\ast)
\end{align*}
Then $\phi$ is given by
\begin{equation*}
    \phi(t, \alpha, \beta) = 
    \frac12
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix}^T
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix}.
\end{equation*}
Denoting
\begin{equation*}\begin{bmatrix}
        \tilde{Q} & \tilde{S}^T\\
        \tilde{S} & \tilde{R}
    \end{bmatrix}
    =
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}^{-1},
\end{equation*}
where
\begin{align}
    &\tilde{Q} = Q^{-1} - Q^{-1} S^T (S Q^{-1} S^T - R)^{-1}S Q^{-1} \label{eq: tilde_q}\\
    &\tilde{R} = R^{-1} - R^{-1} S (S^T R^{-1}S - Q^{-1})^{-1}S^T R^{-1} \label{eq: tilde_r}\\
    &\tilde{S} = (S Q^{-1} S^T - R)^{-1}S Q^{-1} = R^{-1}S(S^TR^{-1}S - Q)^{-1} \label{eq: tilde_s}
\end{align}
we get
\begin{equation}
    \phi(t, \alpha, \beta) = \frac12 \alpha^T \tilde{Q} \alpha + \alpha^T \tilde{S}^T \beta + \frac12 \beta^T \tilde{R} \beta, \label{eq: phi}
\end{equation}
where $\beta = B^T Y + \sum_{i=1}^d D_i^T \beta_i$. Similarly, 
\begin{equation*}
    D_x \big[-x^T y - \frac12 x^T G x - x^T L \big] = -y - Gx - L \implies x^\ast = - G^{-1} (y + L).
\end{equation*}
Then $h(y)$ is given by
\begin{align*}
    h(y) &= (y^T + L^T) G^{-1} y - \frac12 (y^T + L^T)G^{-1}(y + L) + (y^T + L^T) G^{-1} L\\
    &= \frac12 \big[ y^T G^{-1} y +  L^T G^{-1} y + y^T G^{-1} L + L^T G^{-1} L\big]\\
    &= \frac12 (y^T + L^T)G^{-1}(y + L)\numberthis \label{eq: h}
\end{align*}


\section{Unconstrained Markovian Case}


\subsection{Primal Problem}
We now adapt the problem from the last section by introducing a Markov chain, independent of the $d-$dimensional Brownian motion. Let $\eta(t)$ be a continuous-time finite state observable Markov chain. Let the Markov chain take values in the state space $I = \{ 1, 2, \dots, k\}$ and start from an initial state $i_0 \in I$ with a $k \times k$ generator matrix $\mathcal{Q} = \{ {q}_{i j} \}_{i,j = 1}^k$. For each pair of distinct states $(i,j)$ define the counting process $[\mathcal{Q}_{ij}] : \Omega \times [t_0, T] \to \N$ by
\begin{equation*}
    [\mathcal{Q}_{ij}](\omega, t):= \sum_{t_0 < s \le t} \chi_{ \{\eta(s-) = i\}}(\omega) \chi_{\{ \eta(s) = j \}}(\omega), \quad \forall t \in [t_0, T],
\end{equation*}
and the compensator process $\langle \mathcal{Q}_{ij} \rangle: \Omega \times [t_0, T] \to [0, \infty)$ by
\begin{equation*}
    \langle \mathcal{Q}_{ij} \rangle (\omega, t) := q_{ij} \int_{t_0}^t \chi_{\{ \eta(s-)=i \}}(\omega) \d s, \quad \forall t \in [t_0, T].
\end{equation*}
The process
\begin{equation*}
    \mathcal{Q}_{ij}(\omega, t) := [\mathcal{Q}_{ij}](\omega, t) - \langle \mathcal{Q}_{ij} \rangle (\omega, t)
\end{equation*}
is a purely discontinuous square-integrable martingale with an initial value zero.\\

Let $W(t)$ be a $d$-dimensional Brownian motion and consider an $n-$dimensional process $X(t)$ described by
\begin{equation}
    \begin{cases}
        \d X(t) &= b(t, X(t),\pi(t), \eta(t-)) \d t + \sigma(t, x(t), \pi(t), \eta(t-)) \d W(t)\\
         X(t_0) &= x_0 \in \R^n, \, \, \eta(0) = i_0 \in I
    \end{cases}
    \label{eq: markov_sde}
\end{equation}
where $\pi(t) \in \R^m$ is the control and
\begin{align*}
    b&:= A(t, \eta(t-)) X(t) + B(t, \eta(t-)) \pi (t) \in \R^{n}\\
    \sigma &:= 
    \begin{bmatrix}
        \begin{pmatrix}
            \\
            \\
            C_1(t, \eta(t-)) X(t) + D_1(t, \eta(t-)) \pi(t)\\
            \\
            \\
        \end{pmatrix} 
        & \cdots & 
        \begin{pmatrix}
            \\
            \\
            C_d(t, \eta(t-)) X(t) + D_d(t, \eta(t-)) \pi(t)\\
            \\
            \\
        \end{pmatrix}
    \end{bmatrix}
    \in \R^{n \times d}
\end{align*}
where $A, C_i \in \R^{n \times n},$ and $B, D_i \in \R^{n \times m}, i \in \{1, \dots, d \}$ are functions of both time and the Markov chain process.\\

The cost functional is given by
\begin{equation}
    J(\pi) := \E \bigg[ \int_{t_0}^T f(t, X(t), \pi(t), \eta(t)) \d t + g(X(T), \eta(T))\bigg], \label{eq: markov_cost_functional}
\end{equation}
where $f: [t_0, T] \times \R^n \times \R^m \times I \to \R$ and $g: \R^n \to \R$ are given by
\begin{align*}
    f(t, X(t), \pi(t), \eta(t)) &= \frac{1}{2} X^T(t) Q(t, \eta(t)) X(t) + X^T(t) S^T(t, \eta(t)) \pi(t) + \frac{1}{2}\pi^T(t) R(t, \eta(t)) \pi(t)\\
    g(X(T), \eta(T)) &= \frac12 X^T(T) G(T, \eta(T)) X(T) + X^T(T) L(T, \eta(T)).
\end{align*}
The assumptions for this problem are the same to the ones in the previous section. We consider the following optimisation problem
\begin{equation}
    \text{Minimise } J(\pi) \text{ subject to } (X, \pi) \text{ admissible}.  \label{eq: markov_minimisation_problem}
\end{equation}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  DUAL PROBLEM   %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Dual Problem}

We now derive the dual to the primal problem. First, on a probability space $(\Omega, \mathcal{F}, \mathbb{P})$ consider the martingale $M :[t_0, T] \times \Omega \to \R^k$ generated by the Markov process $\eta(t)$:
\begin{equation*}
    M(t) := \eta(t) - \int_{t_0}^t Q^T \eta(s) \d s,
\end{equation*}
where $Q$ is the generating matrix of the Markov chain. Assume that the dual process $Y(t)$ follows
\begin{align}
\begin{cases}
    \d Y(t) &= \bigg[ \alpha(t) - A(t,\eta(t-))^T Y(t) - \sum_{j=1}^d C_j^T(t,\eta(t-)) \beta_j(t)) \bigg]\d t + \sum_{j=1}^d \beta_j(t) \d W_j(t) + \sum_{j=1}^k \gamma_j(t) \d M_j(t)\\
    Y(t_0) &= y
\end{cases}
\label{eq: markov_sde_y}
\end{align}
where $\alpha(t) \in \mathcal{H}([t_0, T], \R^n)$, $\beta_j(t) \in \mathcal{H}([t_0, T], \R^n)$ and $\gamma_j(t) \in \mathcal{H}([t_0, T], \R^n)$ are the dual controls.Using Ito's lemma to $X(t)^T Y(t)$, we get
\begin{align*}
    \d X^T Y &= \bigg[X^T \alpha + \pi^T \beta \bigg] \d t + \sum_{j=1}^d \bigg(X^T \beta_j + Y^T (C_j X + D_j \pi)\bigg) \d W_j,
\end{align*}
where
\begin{equation*}
    \beta = B^T Y +  \sum_{j=1}^d D_j^T \beta_j.
\end{equation*}
The process $X^T(t)Y(t) - \int_{t_0}^t [X^T(s) \alpha(s) + \pi^T(s) \beta(s)] \d s$ is a local martingale and a supermartingale if it is bounded below by an integrable process, which gives 
\begin{equation}
    \E \bigg[ X^T(T) Y(T) - \int_{t_0}^T \big(X^T \alpha + \pi^T \beta \big) \d s \bigg] \le x^T y. \label{eq: markov_dual_1}
\end{equation}
The optimisation problem \eqref{eq: markov_minimisation_problem} can be written equivalently as 
\begin{equation*}
    \sup_\pi \E \bigg[ -\int_{t_0}^T f(t,X(t), \pi(t), \eta(t)) \d t - g(X(T), \eta(T)) \bigg].
\end{equation*}
Define the dual functions $\phi : [t_0, T] \times \R^n \times \R^m \times I \to \R$ by
\begin{equation}
    \phi(t, \alpha, \beta, \eta(t)) = \sup_{x, \pi} \big\{x^T \alpha + \pi^T \beta - f(t, x, \pi, \eta(t)) \big\} \label{eq: markov_phi_1}
\end{equation}
and $h: \R^n \times I \to \R$ by
\begin{equation}
    h(y, \eta(T)) = \sup_x \big\{-x^T y - g(x, \eta(T))\big\}. 
    \label{eq: markov_h_1}
\end{equation}
Substituting $f$ and $g$, we can find the supremums by setting the derivatives to zero. We get
\begin{equation*}
    \phi(t, \alpha, \beta, \eta(t)) = \sup_{x, \pi} \bigg\{
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}^T
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix} - \frac12
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}^T
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}
    \bigg\},
\end{equation*}
so setting the derivative to zero, we get
\begin{equation*}
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix} - 
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}
    \begin{bmatrix}
        x\\
        \pi
    \end{bmatrix}
    = 0 \implies 
    \begin{bmatrix}
        x^\ast\\
        \pi^\ast
    \end{bmatrix} = 
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix}.
\end{equation*}
Therefore
\begin{align*}
    \pi^\ast &= \big[ S Q^{-1}S^T - R \big]^{-1}(S Q^{-1} \alpha - \beta)\\
    x^\ast &= Q^{-1} (\alpha - S^T \pi^\ast)
\end{align*}
Then $\phi$ is given by
\begin{equation*}
    \phi(t, \alpha, \beta,\eta(t)) = 
    \frac12
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix}^T
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}^{-1}
    \begin{bmatrix}
        \alpha\\
        \beta
    \end{bmatrix}. 
\end{equation*}
Denoting
\begin{equation*}\begin{bmatrix}
        \tilde{Q} & \tilde{S}^T\\
        \tilde{S} & \tilde{R}
    \end{bmatrix}
    =
    \begin{bmatrix}
        Q & S^T\\
        S & R
    \end{bmatrix}^{-1},
\end{equation*}
where
\begin{align}
    &\tilde{Q} = Q^{-1} - Q^{-1} S^T (S Q^{-1} S^T - R)^{-1}S Q^{-1} \label{eq: markov_tilde_q}\\
    &\tilde{R} = R^{-1} - R^{-1} S (S^T R^{-1}S - Q^{-1})^{-1}S^T R^{-1} \label{eq: markov_tilde_r}\\
    &\tilde{S} = (S Q^{-1} S^T - R)^{-1}S Q^{-1} = R^{-1}S(S^TR^{-1}S - Q)^{-1} \label{eq: markov_tilde_s}
\end{align}
we get
\begin{equation}
    \phi(t, \alpha, \beta, \eta(t)) = \frac12 \alpha^T \tilde{Q}(t, \eta(t)) \alpha + \alpha^T \tilde{S}^T(t,\eta(t)) \beta + \frac12 \beta^T \tilde{R}(t, \eta(t)) \beta \label{eq: markov_phi}
\end{equation}
Similarly, 
\begin{equation*}
    D_x \big[-x^T y - \frac12 x^T G x - x^T L \big] = -y - Gx - L \implies x^\ast = - G^{-1} (y + L).
\end{equation*}
Then $h(y, \eta(T))$ is given by
\begin{align*}
    h(y, \eta(T)) &= (y^T + L^T) G^{-1} y - \frac12 (y^T + L^T)G^{-1}(y + L) + (y^T + L^T) G^{-1} L\\
    &= \frac12 \big[ y^T G^{-1} y +  L^T G^{-1} y + y^T G^{-1} L + L^T G^{-1} L\big]\\
    &= \frac12 (y^T + L^T)G^{-1}(y + L)\numberthis \label{eq: markov_h}
\end{align*}
Combining \eqref{eq: markov_dual_1}, \eqref{eq: markov_phi_1}, \eqref{eq: h_1}, we get the following inequality:
\begin{equation}
    \sup_\pi \E \bigg[ -\int_{t_0}^T f(t,X,\pi, \eta(t)) \d t - g(X(T), \eta(T)) \bigg] \le \inf_{y, \alpha, \beta_j} \bigg[ x^T y + \E\bigg[\int_{t_0}^T \phi(t,\alpha, \beta, \eta(t) ) \d t + h(Y(T), \eta(T)) \bigg] \bigg] \label{eq: markov_dual_primal_inequality}
\end{equation}
The dual control problem is defined by
\begin{equation}
    \inf_{y, \alpha, \beta_1, \dots, \beta_d} \bigg[ x^T y + \E\bigg[\int_{t_0}^T \phi(t,\alpha, \beta, \eta(t) ) \d t + h(Y(T), \eta(T)) \bigg] \bigg], \label{eq: markov_dual_control_problem}
\end{equation}
This can be solved in two steps: first, for fixed $y$, solve a stochastic control problem
\begin{equation}
    - \tilde{v}(t, y, \eta(t)) = \inf_{ \alpha, \beta_1, \dots, \beta_d} \E\bigg[\int_{t_0}^T \phi (t, \alpha, \beta, \eta(t)) \d t + h(Y(T), \eta(T)) \bigg], \label{eq: markov_dual_value_function}
\end{equation}
and second, solve a static optimisation problem
\begin{equation*}
    \inf_y \big\{x^T y - \tilde{v}(t,y, \eta(t))\big\}.
\end{equation*}


\section{Constained Non-Markovian Case}

\subsection{Primal Problem}
 For simplicity, we consider the case with one-dimensional Brownian motion, or $d=1$. The state process is then given by the following SDE:
\begin{equation}
    \begin{cases}
    \d X(t) &= \big[A(t) X(t) + B(t) \pi(t)  \big] \d t + \big[C(t) X(t) + D(t) \pi(t)  \big] \d W(t)\\
    X(t_0) &= x_0 \in \R^n.
    \end{cases} \label{eq: constrained_x_sde}
\end{equation}
So far, we have not imposed any constraints on the control $\pi(t)$, i.e. we had $\pi(t) \in \R^m$. Now, we consider the case where $\pi(t) \in K \subset \R^m$, where $K$ is a convex set.\\

In the previous sections, we also had quadratic functions $f$ and $g$. Now we relax these conditions and consider a more general case where $f: \Omega \times [t_0, T] \times \R^n \times \R^m \to \R$ and $g: \Omega \times \R^n \to \R$ are measurable functions, $f$ being $\mathbb{F}$-progressively measurable for fixed $(x, \pi)$, convex in $(x, \pi)$, $C^1$ in $x$, continuous in $\pi$, and $g$ being $\mathcal{F}_T$-measurable for fixed $x$, convex and $C^1$ in $x$.\\

The set of admissible controls in this section is defined as
\begin{equation*}
    \mathcal{A}:= \{ \pi \in \mathcal{H}([t_0, T], \R^m) : \pi(t) \in K \text{ for } t \in [t_0, T], a.e.  \}.
\end{equation*}
The optimisation problem is defined as 
\begin{equation}
    \inf_{\pi \in K} \E \bigg[ \int_{t_0}^T f(t,X(t), \pi(t)) \d t + g(X(T)) \bigg].\label{eq: constrained_primal_optimisation_problem}
\end{equation}

If we denote by $\Tilde{f}(t,x,\pi)$ the function
\begin{equation*}
    \Tilde{f}(t,x,\pi) = f(t,x,\pi) + \Psi_K(\pi),
\end{equation*}
where 
\begin{equation*}
    \Psi_K(\pi) = \begin{cases}
        0 &\text{if } \pi \in K\\
        + \infty &\text{if } \pi \not \in K.
    \end{cases}
\end{equation*}
then the optimisation problem can be formulated as
\begin{equation}
    \sup_{\pi \in \R^m} \E \bigg[ - \int_{t_0}^T \Tilde{f}(t,X,\pi) \d t -  g(X(T))\bigg]. \label{eq: constrained_primal_optimisation_problem_tilde}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%% DUAL PROBLEM %%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Dual Problem}
We now derive the dual problem in a similar way as before (adapted from \cite{Constrained_Quadratic_Risk_Minimisation_Via_FBSDEs}). Assume that the dual process $Y$ is driven by an SDE
\begin{equation*}
    \d Y(t) = \alpha_1(t) \d t + \beta_1(t) \d W(t),
\end{equation*}
with initial condition $Y(t_0) = y$, where $\alpha_1(t) \in \R^n$ and $\beta_1(t) \in \R^n$ are two stochastic processes to be determined. Ito's lemma gives
\begin{equation*}
    \d (X^T Y) = \big[ X^T \underbrace{(\alpha_1 + A^T Y + C^T \beta_1)}_{:= \alpha} + \pi^T \underbrace{(B^T Y + D^T \beta_1)}_{:= \beta}\big] \d t + \text{local martingale}.
\end{equation*}
Denoting $\alpha = \alpha_1 + A^T Y + C^T \beta_1$ and $\beta = B^T Y + D^T \beta_1$, we have that the dual process $Y(t)$ satisfies:
\begin{equation}
    \begin{cases}
        \d Y(t) &= \big[ \alpha(t) - A(t)^T Y(t) - C^T(t) \beta_1(t)\big]\d t + \beta_1(t) \d W(t)\\
        Y(t_0) &= y, \label{eq: constrained_y_sde}
    \end{cases}
\end{equation}
Returning to Ito's lemma to $X(t)^T Y(t)$, we get
\begin{align*}
    \d X^T Y &= (X^T \alpha + \pi^T \beta ) \d t + \text{local martingale}
\end{align*}
where
\begin{equation}
    \beta = B^T Y +  D^T \beta_1. \label{eq: constrained_beta_relation}
\end{equation}
The process $X^T(t)Y(t) - \int_{t_0}^t [X^T(s) \alpha(s) + \pi^T(s) \beta(s)] \d s$ is a local martingale and a supermartingale if it is bounded below by an integrable process, which gives 
\begin{equation}
    \E \bigg[ X^T(T) Y(T) - \int_{t_0}^T \big(X^T \alpha + \pi^T \beta \big) \d s \bigg] \le x^T y. \label{eq: constrained_dual_1}
\end{equation}
Define the dual functions $\phi : [t_0, T] \times \R^n \times \R^m \to \R$ by
\begin{equation}
    \phi(t, \alpha, \beta) = \sup_{x, \pi} \big\{x^T \alpha + \pi^T \beta - \tilde{f}\/(t, x, \pi) \big\} \label{eq: constrained_phi_1}
\end{equation}
and $h: \R^n \to \R$ by
\begin{equation}
    h(y) = \sup_x \big\{-x^T y - g(x)\big\}. 
    \label{eq: constrained_h_1}
\end{equation}
We have $\phi$ and $h$ are proper closed convex functions \cite[Proposition 1.1.6, Proposition 1.6.1]{Convex_Optimisation_Theory}.\\

Combining \eqref{eq: constrained_dual_1}, \eqref{eq: constrained_phi_1} and \eqref{eq: constrained_h_1} we get the following inequality:
\begin{equation*}
    \sup_{\pi} \E \bigg[ - \int_{t_0}^T \Tilde{f}(t, X, \pi) \d t - g(X(T)) \bigg] \le \inf_{y, \alpha, \beta_1} \bigg\{ x^T y + \E \bigg[ \int_{t_0}^T \phi(t, \alpha, \beta) \d t + h(Y(T)) \bigg]  \bigg\}.
\end{equation*}
The dual control problem is defined by
\begin{equation}
    \inf_{y, \alpha, \beta_1} \bigg\{ x^T y + \E \bigg[ \int_{t_0}^T \phi(t, \alpha, \beta) \d t + h(Y(T)) \bigg]  \bigg\}, \label{eq: constrain_dual_control_problem}
\end{equation}
where $Y$ satisfies \eqref{eq: constrained_y_sde}. Problem \eqref{eq: constrain_dual_control_problem} can be solved in two steps: first, for fixed $y$ solve a stochastic control problem
\begin{equation}
    - \tilde{v}(y) = \inf_{\alpha, \beta_1} \E \bigg[ \int_{t_0}^T \phi(t, \alpha, \beta) \d t + h(Y(T)) \bigg] \label{eq: constrained_dual_value_function_1}
\end{equation}
and second, solve a static optimisation problem:
\begin{equation}
    \inf_y \big \{ x^T y + V(y)  \big \}.
\end{equation}


